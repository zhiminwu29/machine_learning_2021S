---
title: 'STP 598: Homework 4'
author: "Antonio Campbell, Sinta Sulistyo, Atta Ullah, Penny Wu"
date: "3/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Problem 4.Regularized Regression

Letâ€™s try ridge and LASSO on the car price data.
The cars data has 20 thousand observations and 11 variables.
Many of the x variables are categorial so we need to dummy them up.

```{r}
cd = read.csv("http://www.rob-mcculloch.org/data/usedcars.csv")
print(dim(cd))
head(cd)

library(glmnet)
```

```{r}
#dummy up the categorical variables
cd$trim=as.factor(cd$trim)
cd$isOneOwner=as.factor(cd$isOneOwner)
cd$color=as.factor(cd$color)
cd$displacement=as.factor(cd$displacement)
cd$fuel=as.factor(cd$fuel)
cd$region=as.factor(cd$region)
cd$soundSystem=as.factor(cd$soundSystem)
cd$wheelType=as.factor(cd$wheelType)

summary(cd)
```

```{r}
## Get the model matrix (x) and log price (y)
x=model.matrix(log(price)~.,cd)[,-1]
x=scale(x) #let's scale the data to start with so we an interpret stuff.
y=log(cd$price)
print(dim(x))
print(colnames(x))
```

# (a). Use the LASSO to relate log of price to the features.
```{r}
## do lasso (alpha=1)
gsize=100
grid=5^seq(10,-2,length=gsize)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid,standardize=FALSE)  
```

```{r}
## plot coefficients
cmat = coef(lasso.mod) 
cmat = cmat[2:nrow(cmat),] #drop intercept
rgy = range(cmat)
cpar = log(1/grid)
plot(range(cpar),rgy,type='n',xlab='log(1/lambda)',ylab='coeficients',cex.lab=1.5)
for(i in 1:nrow(cmat)) lines(cpar,cmat[i,],col=i+1,type='l')
```
It is clearly seen from the graph above, there ar several variables with zero coefficient, and as lambda gets smaller - meaning that the penalty gets lower - the coefficients increase. 

```{r}
##  Try Cross validation using cv.glmnet
set.seed(80)
cv.out = cv.glmnet(x,y,alpha=1,lambda=grid)
cmp = log(1/cv.out$lambda)

plot(cmp,cv.out$cvm,type='b',xlab='cmp = log(1/lambda)',cex.lab=1.5)
bestlam = cv.out$lambda.min
bestcmp = log(1/bestlam)
text(bestcmp,160000,paste('best lambda is: ',round(bestlam,2)),col='red',cex=1.5)
abline(v=bestcmp,col='red')
```
```{r}
print(bestlam)
```
Based on the cross validation, the best lambda is 0.04.

```{r}
##get coefficients for best lambda
bestlassocoef = predict(lasso.mod,s=bestlam,type='coefficients',exact=TRUE)[,1]
ddf = data.frame(x,y)
lm.mod = lm(y~.,ddf)
rgxy = range(c(lm.mod$coeficients,bestlassocoef))
plot(lm.mod$coef,bestlassocoef,xlim=rgxy,ylim=rgxy,xlab='linear coefficients',ylab='lasso coefficients')
abline(0,1,col='red',lty=2)
abline(v=0,lty=3)
abline(h=0,lty=3)
```
Acknowledging that lambda is relatively small, it is almost similar with linear regression. Hence, the coefficients of lasso and linear regression are very close. 

```{r}
print(bestlassocoef)
```
As shown above, some variables have 0 coefficient, such as trim350, trim 400, trim420.

```{r}
##get fits
lasso.fit = predict(lasso.mod,s=bestlam,newx=x)
lm.fit = lm.mod$fitted
fmat = cbind(y,lm.fit,lasso.fit)
colnames(fmat) = c('y','linear','lasso')
pairs(fmat)
```
Comparing the fit between linear and lasso with lambda = 0.04, it is pretty close. 

# (b). Use ridge regression to relate log of price to the features.
```{r}
## Fit ridge on grid of lambda values.
gsize=100
grid=5^seq(10,-2,length=gsize)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid,standardize=FALSE)  #I already standardized
```

```{r}
## plot coefficients
cmat = coef(ridge.mod) 
cmat = cmat[2:nrow(cmat),] #drop intercept
rgy = range(cmat)
cpar = log(1/grid)
plot(range(cpar),rgy,type='n',xlab='log(1/lambda)',ylab='coeficients',cex.lab=1.5)
for(i in 1:nrow(cmat)) lines(cpar,cmat[i,],col=i+1,type='l')

```
The graph shows the coefficient value of each variables as lambda becomes smaller. Most of the variables' coefficient get bigger as the lambda gets smaller. This makes sense since when lambda is small, the penalty is low. 

```{r}
##  Try Cross validation using cv.glmnet
set.seed(14)
cv.out = cv.glmnet(x,y,alpha=0,lambda=grid)
cmp = log(1/cv.out$lambda)

plot(cmp,cv.out$cvm,type='b',xlab='cmp = log(1/lambda)',cex.lab=1.5)
bestlam = cv.out$lambda.min
bestcmp = log(1/bestlam)
text(bestcmp,160000,paste('best lambda is: ',round(bestlam,2)),col='red',cex=1.5)
abline(v=bestcmp,col='red')
```
```{r}
print(bestlam)
```
Based on the cross validation, the best lambda for ridge regression is 0.04. 

```{r}
##get coefficients for best lambda
bestridgecoef = predict(ridge.mod,s=bestlam,type='coefficients',exact=TRUE)[,1]
rgxy = 1.1*range(c(lm.mod$coeficients,bestridgecoef,bestlassocoef))
plot(lm.mod$coef,bestridgecoef,xlim=rgxy,ylim=rgxy,xlab='linear coefficients',ylab='ridge-lasso coefficients',col='green',pch=2,cex.lab=1.5)
points(lm.mod$coef,bestlassocoef,col='blue',pch=19)
abline(0,1,col='red',lty=2)
abline(v=0,lty=3)
abline(h=0,lty=3)
legend('topleft',legend=c('ridge','lasso'),pch=19,col=c('green','blue'))
```

```{r}
print(bestridgecoef)
```
Using ridge regression, there is no variable selection since all of the variables' coefficient is not zero.

```{r}
##get fits
ridge.fit = predict(ridge.mod,s=bestlam,newx=x)
fmat = cbind(y,lm.fit,ridge.fit,lasso.fit)
colnames(fmat) = c('y','linear','ridge','lasso')
pairs(fmat)
```
The plot above shows the comparison between linear, ridge, and lasso. 
