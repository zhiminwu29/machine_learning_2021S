---
title: 'STP 598: Homework 3'
author: "Antonio Campbell, Sinta Sulistyo, Atta Ullah, Penny Wu"
date: "3/9/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
#=============================================
## Clear .Rdata
rm(list=ls())

## Packages
pkg.list <-c("ggplot2", "reshape2", "knitr","matrixcalc",
             "rootSolve", "Matrix", "kknn", "Metrics", "pracma"
             ,"glmnet")

#install.packages(pkg.list)
lapply(pkg.list, require, character.only = TRUE)

knitr::opts_chunk$set(echo = FALSE, fig.align='center')
#=============================================
```

# 1. Problem 1. Basic Optimization, MLE for IID Poisson Data
Suppose $y_i$ is a count then a very common model is to assume the Poisson disttribuion: $$P(Y=y \;|\; \lambda) = \frac{e^{-\lambda} \, \lambda^y}{y!}, \; y = 0,1,2,\ldots$$
Given $Y_i \sim Poisson(\lambda)$ iid, (that is, \(Y_i = y_i\)), what is the MLE of $\lambda$?

If the random variables are iid, then the joint distribution is the product of the marginal distributions. Let $\mathbf{y}$ be the sample $y_1, \dots, y_n$ 
$$L(\lambda \vert \mathbf{y}) = \prod_{i = 1}^{n}\frac{e^{-\lambda} \, \lambda^{y_i}}{y_i!}.$$ We take the log of the likelihood so that we can maximize easily. 
$$\log(L(\lambda \vert \mathbf{y})) = \sum_{i = 1}^{n}\log\bigg(\frac{e^{-\lambda} \, \lambda^{y_i}}{y_i!}\bigg) = -n\lambda -\sum{\log(y_i!)}+ \sum(y_i)\log(\lambda).$$
Differentiate with respect to lambda and set to zero to maximize: $$\frac{\delta \log(L(\lambda \vert \mathbf{y}))}{\delta d\lambda}=-n + \frac{\sum y_i}{\lambda} = 0 \implies \hat\lambda =\frac{\sum y_i}{n}$$. 

The MLE for the parameter of the $poisson$ distribution is the sample mean. 


# Problem 2. Constrained Optimization, Minimum Variance Portfolio

Suppose we are considering investing in $p$ stocks where the uncertain return on the $i$th
stock is denoted by $R_i$, $i=1,2,\dots,p$. Let $R=(R_1,R_2,\dots,R_p)^{'}.$ A portfolio is a given by $w=(w_1,w_2,\dots,w_p)^{'}$ where $w_i$ is the fraction of wealth invested in asset $i$. The $\{w_i\}$
must satisfy $\sum w_i=1$. The return on the portfolio is then $$P = w'R = \sum w_i R_i.$$ We want to find the global \textbf{minimum variance portfolio}: $$\underset{w}{\min} \, Var(P), \;\; \text{subject to} \sum w_i = 1.$$ If we let $\iota=(1,1,\dots,1)^{'}$, the vector of ones, and $Var(R)=\Sigma$ then our problem is $$\underset{w}{\min }\,  w^{'}\Sigma w  \;\; \text{subject to } w^{'}\iota = 1.$$ Find the global minimum variance portfolio in terms of $\Sigma$
and $\iota$.

If we know that $\sum {w_i} = 1$ we can easily say $\sum {w_i}-1 = 0$ or $w^{'}\iota-1 = 0$. We use the Karush-Kuhn-Tucker conditions to minimize. By this we take the derivative with respect to $w$, $$\frac{\delta  L(\lambda,w)}{\delta w} = \frac{\delta  (w^{'}\Sigma{w})}{\delta w} + \frac{\delta  \lambda(w^{'}\iota-1)}{\delta w} = 2\Sigma w + \lambda\iota$$ $$ \implies {\hat w} = -\lambda\Sigma^{-1}\iota/2.$$ To express this solution in terms of $\Sigma, \iota$ we solve for $\lambda$. Premultiply by $\iota^{'}$ and then solve. Since we have that $\iota^{'}{w} = 1$, $$ \iota^{'}{w} = -\lambda\iota^{'}\Sigma^{-1}\iota/2  \implies \lambda = -2/(\iota^{'}\Sigma^{-1}{\iota}).$$ We plug this into our solution of $\hat w$ to have:

$$\hat{w} = \frac{-\lambda\Sigma^{-1}\iota/2}{-2/\iota^{'}\Sigma^{-1}{\iota}} =\frac{\lambda\Sigma^{-1}\iota}{\iota^{'}\Sigma^{-1}{\iota}}$$

$\hat w$ is the $w$ such that $Var(w^{'}R) = Var(P)$ is minimized.


# Problem 3. Polynomial Regression

A basic idea in nonlinear regression is to use polynomial terms.

With one $x$ variable, this means we consider the models: $$Y_i=\beta_0+\beta_1 x_i+\beta_2 x^2_i+\ldots+\beta_p x^p_i+\epsilon_i.$$

Using the simple used cars data (with $n=1,000$) with $Y$= price and $x$=mileage, find the best choice of $p$. 
* Use BIC to pick $p$
* use an out-of-sample criteria to pick $p$

Fit your chosen polynomial mode using all the data and plot the fit on top of the data. Do you like it? Also plot the fits for a $p$ that is “way to big”. What is wrong with it?



## Solution

###  Impriting libraries and loading dataset
```{r}
library(leaps) 
dpl= FALSE 
library(ISLR) 
data_fram  = read.csv("http://www.rob-mcculloch.org/data/usedcars.csv")
head(data_fram)
```

We only need the price and mileage and various powers of mileage. We create a new dataset with the first column as the price of the following rest of the columns are the mileage with exponent 1 through 9. 

```{r}
p = 9
data_fram2 <- data.frame(matrix(NA,nrow=1000, ncol=10))
data_fram2[,1:2] <- data_fram[1:1000,c("price", "mileage")]
for(i in 3:ncol(data_fram2)) data_fram2[,i] <- data_fram2[,2]^(i-1)
colnames(data_fram2) <- c("price", "mileage", paste("mileage", 2:9,sep = ""))
head(data_fram2,3)

```
## Plot of R-Squared Error

```{r}
regfit.best = regsubsets(price~., data = data_fram2,  nvmax=19 ,nbest=1,method="exhaustive") 
regsum = summary(regfit.best)
regsum$which

print(regsum$which[1:9,])

if(dpl) pdf(file = 'UsedCars_rsq_alldat.pdf',height=10,width=12)
plot(regsum$rsq,xlab='num var (k)',ylab='R-squared',cex.lab=1.5,type='b',col='blue')
if(dpl) dev.off()

```

As expected, r-sqaured error is monotonically increasing as the number of variables increase. 

The following function gives the coefficients for any choice of variables. For instance, if we happen to choose the best subset with 5 varaibles, we use

```{r}
print(regsum$which[1:5,])
print(coef(regfit.best,3))
```
## function to do rmse for $k$ in $1:p$

```{r}
dovalbest = function(object,newdata,ynm)
  #object: regsubsets on train
  #newdata: test data
  #ynm: name of y in data frame.
{
  form = as.formula(object$call[[2]])
  p=ncol(newdata)-1
  rmsev = rep(0,p)
  test.mat = model.matrix(form,newdata)
  for(k in 1:p) {
    coefk = coef(object,id=k)
    xvars = names(coefk)
    pred = test.mat[,xvars] %*% coefk
    rmsev[k] = sqrt(mean((newdata[[ynm]]-pred)^2))
  }
  return(rmsev)
}

```

## Do validation approach several times

```{r}

ntry=100
p=ncol(data_fram2)-1
resmat = matrix(0,p,ntry) 
set.seed(18)
for(i in 1:ntry) {
  train = sample(1:nrow(data_fram2),floor(nrow(data_fram2)/2))
  regfit.best=regsubsets(price~.,data=data_fram2[train,],nvmax=19,nbest=1,method="exhaustive")
  resmat[,i]=dovalbest(regfit.best,data_fram2[-train,],'price')
}

```
## Plot results of repeated train/val

```{r}
mresmat = apply(resmat,1,mean) #average across columns
if(dpl) pdf(file='usedCars_train-val.pdf',height=10,width=12)
plot(mresmat,xlab='num vars',ylab='rmse',type='b',col='blue',pch=19,cex.lab=1.5)
if(dpl) dev.off()
```
## Fit using number of vars chosen by train/validation and all the data.

```{r}
kopt = 3 #optimal k=number of vars: chosen by eye-balling plot
regfit.best=regsubsets(price~.,data=data_fram2,nvmax=kopt,nbest=1,method="exhaustive")
xmat = model.matrix(data_fram2$price~.,data_fram2)
ddf = data.frame(xmat[,-1],price=data_fram2$price) #don't use intercept (-1), and in y=price
nms = c(names(coef(regfit.best,kopt))[-1],"price")
ddfsub = ddf[,nms] #drop all vars except those names by the coef at kopt
thereg = lm(price~.,ddfsub)
print(summary(thereg))
```

## BIC

```{r}
regfit.best=regsubsets(price~.,data=data_fram2,nvmax=9,nbest=1,method="exhaustive")
if(dpl) pdf(file="bic-best-on-train_usedCars.pdf",height=8,width=7)
plot(regfit.best) 
#will plot models ordered by BIC (the default)

for(i in 1:ncol(data_fram2)) abline(v=i,col="red",lty=2) 
#so you can see which var it is.
if(dpl) dev.off()

sumreg = summary(regfit.best)
if(dpl) pdf(file="usedCars-BIC.pdf",height=10,width=12)
plot(sumreg$bic,xlab="k",ylab="BIC",type='b',col="blue",lwd=2,cex.lab=1.5)
if(dpl) dev.off()
```
## Ploting the model with the best coice of number of variables.

### Using BIC and Cross validation.

By eyeballing we see that the best choice of $p$ for both is 3.
```{r}
print(coef(regfit.best,3)) #coefficients for best model with 2 x variables
```
Thus the best polynomila would be

$$
Y_i=\beta_0+\beta_1 x_i+\beta_2x^2_i+\beta_4x^4_i
$$
Where 
$$
\beta_0 = 7.176771 \times 10^{04} \\
\beta_1 = -8.152826\times 10^{-01}\\
\beta_2 = 2.771420\times 10^{-06} \\
\beta_4 = -8.457424\times 10^{-18}
$$

We plot this polynomial with data
```{r}
fittedmodel = 7.176771e+04 +(-8.152826e-01)*data_fram2$mileage+
  (2.771420e-06)*data_fram2$mileage2 + 
  (-8.457424e-18)*data_fram2$mileage4

plot(data_fram2$mileage,data_fram2$price)
par(new=TRUE)
plot(data_fram2$mileage,fittedmodel,col ="red")
```
This looks good except for the mileage from 150000 to 20000 is a bit off. \

### Using a "way to big" $p$. 
Since in our data frame we have used the maximum degree 9, we will fit a polynomial of degree 9. 

```{r}
model_with_big_P =  6.265684e+04  + 1.835415e-01*(data_fram2$mileage)-
  3.725474e-05*(data_fram2$mileage2)+8.178492e-10*(data_fram2$mileage3)-
  9.556166e-15*(data_fram2$mileage4)+6.534804e-20*(data_fram2$mileage5)-
  2.535158e-25*(data_fram2$mileage6) + 4.917961e-31*(data_fram2$mileage7)-
  2.823111e-37*(data_fram2$mileage8)-2.254280e-43*(data_fram2$mileage9) 
plot(data_fram2$mileage, data_fram2$price)
par(new=TRUE)
plot(data_fram2$mileage, model_with_big_P,col ="red")
```

This one is not as good as the previous and slight off for most of the data but does capture the points near the boundary vary well. 


# Problem 4.Regularized Regression

Let’s try ridge and LASSO on the car price data.
The cars data has 20 thousand observations and 11 variables.
Many of the x variables are categorial so we need to dummy them up.

```{r}
cd = read.csv("http://www.rob-mcculloch.org/data/usedcars.csv")
print(dim(cd))
head(cd)
library(glmnet)
```

```{r}
#dummy up the categorical variables
cd$trim=as.factor(cd$trim)
cd$isOneOwner=as.factor(cd$isOneOwner)
cd$color=as.factor(cd$color)
cd$displacement=as.factor(cd$displacement)
cd$fuel=as.factor(cd$fuel)
cd$region=as.factor(cd$region)
cd$soundSystem=as.factor(cd$soundSystem)
cd$wheelType=as.factor(cd$wheelType)
summary(cd)
```

```{r}
## Get the model matrix (x) and log price (y)
x=model.matrix(log(price)~.,cd)[,-1]
x=scale(x) #let's scale the data to start with so we an interpret stuff.
y=log(cd$price)
print(dim(x))
print(colnames(x))
```

# (a). Use the LASSO to relate log of price to the features.
```{r}
## do lasso (alpha=1)
gsize=100
grid=5^seq(10,-2,length=gsize)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid,standardize=FALSE)  
```

```{r}
## plot coefficients
cmat = coef(lasso.mod) 
cmat = cmat[2:nrow(cmat),] #drop intercept
rgy = range(cmat)
cpar = log(1/grid)
plot(range(cpar),rgy,type='n',xlab='log(1/lambda)',ylab='coeficients',cex.lab=1.5)
for(i in 1:nrow(cmat)) lines(cpar,cmat[i,],col=i+1,type='l')
```
It is clearly seen from the graph above, there ar several variables with zero coefficient, and as lambda gets smaller - meaning that the penalty gets lower - the coefficients increase. 

```{r}
##  Try Cross validation using cv.glmnet
set.seed(80)
cv.out = cv.glmnet(x,y,alpha=1,lambda=grid)
cmp = log(1/cv.out$lambda)
plot(cmp,cv.out$cvm,type='b',xlab='cmp = log(1/lambda)',cex.lab=1.5)
bestlam = cv.out$lambda.min
bestcmp = log(1/bestlam)
text(bestcmp,160000,paste('best lambda is: ',round(bestlam,2)),col='red',cex=1.5)
abline(v=bestcmp,col='red')
```
```{r}
print(bestlam)
```
Based on the cross validation, the best lambda is 0.04.

```{r}
##get coefficients for best lambda
bestlassocoef = predict(lasso.mod,s=bestlam,type='coefficients',exact=TRUE)[,1]
ddf = data.frame(x,y)
lm.mod = lm(y~.,ddf)
rgxy = range(c(lm.mod$coeficients,bestlassocoef))
plot(lm.mod$coef,bestlassocoef,xlim=rgxy,ylim=rgxy,xlab='linear coefficients',ylab='lasso coefficients')
abline(0,1,col='red',lty=2)
abline(v=0,lty=3)
abline(h=0,lty=3)
```
Acknowledging that lambda is relatively small, it is almost similar with linear regression. Hence, the coefficients of lasso and linear regression are very close. 

```{r}
print(bestlassocoef)
```
As shown above, some variables have 0 coefficient, such as trim350, trim 400, trim420.

```{r}
##get fits
lasso.fit = predict(lasso.mod,s=bestlam,newx=x)
lm.fit = lm.mod$fitted
fmat = cbind(y,lm.fit,lasso.fit)
colnames(fmat) = c('y','linear','lasso')
pairs(fmat)
```
Comparing the fit between linear and lasso with lambda = 0.04, it is pretty close. 

# (b). Use ridge regression to relate log of price to the features.
```{r}
## Fit ridge on grid of lambda values.
gsize=100
grid=5^seq(10,-2,length=gsize)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid,standardize=FALSE)  #I already standardized
```

```{r}
## plot coefficients
cmat = coef(ridge.mod) 
cmat = cmat[2:nrow(cmat),] #drop intercept
rgy = range(cmat)
cpar = log(1/grid)
plot(range(cpar),rgy,type='n',xlab='log(1/lambda)',ylab='coeficients',cex.lab=1.5)
for(i in 1:nrow(cmat)) lines(cpar,cmat[i,],col=i+1,type='l')
```
The graph shows the coefficient value of each variables as lambda becomes smaller. Most of the variables' coefficient get bigger as the lambda gets smaller. This makes sense since when lambda is small, the penalty is low. 

```{r}
##  Try Cross validation using cv.glmnet
set.seed(14)
cv.out = cv.glmnet(x,y,alpha=0,lambda=grid)
cmp = log(1/cv.out$lambda)
plot(cmp,cv.out$cvm,type='b',xlab='cmp = log(1/lambda)',cex.lab=1.5)
bestlam = cv.out$lambda.min
bestcmp = log(1/bestlam)
text(bestcmp,160000,paste('best lambda is: ',round(bestlam,2)),col='red',cex=1.5)
abline(v=bestcmp,col='red')
```
```{r}
print(bestlam)
```
Based on the cross validation, the best lambda for ridge regression is 0.04. 

```{r}
##get coefficients for best lambda
bestridgecoef = predict(ridge.mod,s=bestlam,type='coefficients',exact=TRUE)[,1]
rgxy = 1.1*range(c(lm.mod$coeficients,bestridgecoef,bestlassocoef))
plot(lm.mod$coef,bestridgecoef,xlim=rgxy,ylim=rgxy,xlab='linear coefficients',ylab='ridge-lasso coefficients',col='green',pch=2,cex.lab=1.5)
points(lm.mod$coef,bestlassocoef,col='blue',pch=19)
abline(0,1,col='red',lty=2)
abline(v=0,lty=3)
abline(h=0,lty=3)
legend('topleft',legend=c('ridge','lasso'),pch=19,col=c('green','blue'))
```

```{r}
print(bestridgecoef)
```
Using ridge regression, there is no variable selection since all of the variables' coefficient is not zero.

```{r}
##get fits
ridge.fit = predict(ridge.mod,s=bestlam,newx=x)
fmat = cbind(y,lm.fit,ridge.fit,lasso.fit)
colnames(fmat) = c('y','linear','ridge','lasso')
pairs(fmat)
```
The plot above shows the comparison between linear, ridge, and lasso. 