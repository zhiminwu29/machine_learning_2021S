---
title: 'STP 598: Homework 3'
author: "Antonio Campbell, Sinta Sulistyo, Atta Ullah, Penny Wu"
date: "3/9/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
#=============================================
## Clear .Rdata
rm(list=ls())

## Packages
pkg.list <-c("ggplot2", "reshape2", "knitr","matrixcalc",
             "rootSolve", "Matrix", "kknn", "Metrics", "pracma"
             ,"glmnet")

#install.packages(pkg.list)
lapply(pkg.list, require, character.only = TRUE)

knitr::opts_chunk$set(echo = FALSE, fig.align='center')
#=============================================
```

#1. Problem 1. Basic Optimization, MLE for IID Poisson Data
Suppose $y_i$ is a count then a very common model is to assume the Poisson disttribuion: $$P(Y=y \;|\; \lambda) = \frac{e^{-\lambda} \, \lambda^y}{y!}, \; y = 0,1,2,\ldots$$
Given $Y_i \sim Poisson(\lambda)$ iid, (that is, \(Y_i = y_i\)), what is the MLE of $\lambda$?

If the random variables are iid, then the joint distribution is the product of the marginal distributions. Let $\mathbf{y}$ be the sample $y_1, \dots, y_n$ 
$$L(\lambda \vert \mathbf{y}) = \prod_{i = 1}^{n}\frac{e^{-\lambda} \, \lambda^{y_i}}{y_i!}.$$ We take the log of the likelihood so that we can maximize easily. 
$$\log(L(\lambda \vert \mathbf{y})) = \sum_{i = 1}^{n}\log\bigg(\frac{e^{-\lambda} \, \lambda^{y_i}}{y_i!}\bigg) = -n\lambda -\sum{\log(y_i!)}+ \sum(y_i)\log(\lambda).$$
Differentiate with respect to lambda and set to zero to maximize: $$\frac{\delta \log(L(\lambda \vert \mathbf{y}))}{\delta d\lambda}=-n + \frac{\sum y_i}{\lambda} = 0 \implies \hat\lambda =\frac{\sum y_i}{n}$$. 

The MLE for the parameter of the $poisson$ distribution is the sample mean. 


#Problem 2. Constrained Optimization, Minimum Variance Portfolio

Suppose we are considering investing in $p$ stocks where the uncertain return on the $i$th
stock is denoted by $R_i$, $i=1,2,\dots,p$. Let $R=(R_1,R_2,\dots,R_p)^{'}.$ A portfolio is a given by $w=(w_1,w_2,\dots,w_p)^{'}$ where $w_i$ is the fraction of wealth invested in asset $i$. The $\{w_i\}$
must satisfy $\sum w_i=1$. The return on the portfolio is then $$P = w'R = \sum w_i R_i.$$ We want to find the global \textbf{minimum variance portfolio}: $$\underset{w}{\min} \, Var(P), \;\; \text{subject to} \sum w_i = 1.$$ If we let $\iota=(1,1,\dots,1)^{'}$, the vector of ones, and $Var(R)=\Sigma$ then our problem is $$\underset{w}{\min }\,  w^{'}\Sigma w  \;\; \text{subject to } w^{'}\iota = 1.$$ Find the global minimum variance portfolio in terms of $\Sigma$
and $\iota$.

If we know that $\sum {w_i} = 1$ we can easily say $\sum {w_i}-1 = 0$ or $w^{'}\iota-1 = 0$. We use the Karush-Kuhn-Tucker conditions to minimize. By this we take the derivative with respect to $w$, $$\frac{\delta  L(\lambda,w)}{\delta w} = \frac{\delta  (w^{'}\Sigma{w})}{\delta w} + \frac{\delta  \lambda(w^{'}\iota-1)}{\delta w} = 2\Sigma w + \lambda\iota$$ $$ \implies {\hat w} = -\lambda\Sigma^{-1}\iota/2.$$ To express this solution in terms of $\Sigma, \iota$ we solve for $\lambda$. Premultiply by $\iota^{'}$ and then solve. Since we have that $\iota^{'}{w} = 1$, $$ \iota^{'}{w} = -\lambda\iota^{'}\Sigma^{-1}\iota/2  \implies \lambda = -2/(\iota^{'}\Sigma^{-1}{\iota}).$$ We plug this into our solution of $\hat w$ to have:

$$\hat{w} = \frac{-\lambda\Sigma^{-1}\iota/2}{-2/\iota^{'}\Sigma^{-1}{\iota}} =\frac{\lambda\Sigma^{-1}\iota}{\iota^{'}\Sigma^{-1}{\iota}}$$

$\hat w$ is the $w$ such that $Var(w^{'}R) = Var(P)$ is minimized.


# Problem 3. Polynomial Regression

A basic idea in nonlinear regression is to use polynomial terms.

With one $x$ variable, this means we consider the models: $$Y_i=\beta_0+\beta_1 x_i+\beta_2 x^2_i+\ldots+\beta_p x^p_i+\epsilon_i.$$

Using the simple used cars data (with $n=1,000$) with $Y$= price and $x$=mileage, find the best choice of $p$. 
* Use BIC to pick $p$
* use an out-of-sample criteria to pick $p$

Fit your chosen polynomial mode using all the data and plot the fit on top of the data. Do you like it? Also plot the fits for a $p$ that is “way to big”. What is wrong with it?



## Solution

###  Impriting libraries and loading dataset
```{r}
library(leaps) 
dpl= FALSE 
library(ISLR) 
data_fram  = read.csv("http://www.rob-mcculloch.org/data/usedcars.csv")
head(data_fram)
```

We only need the price and mileage and various powers of mileage. We create a new dataset with the first column as the price of the following rest of the columns are the mileage with exponent 1 through 9. 

```{r}
p = 9
data_fram2 <- data.frame(matrix(NA,nrow=1000, ncol=10))
data_fram2[,1:2] <- data_fram[1:1000,c("price", "mileage")]
for(i in 3:ncol(data_fram2)) data_fram2[,i] <- data_fram2[,2]^(i-1)
colnames(data_fram2) <- c("price", "mileage", paste("mileage", 2:9,sep = ""))
head(data_fram2)

```
## Plot of R-Squared Error

```{r}
regfit.best = regsubsets(price~., data = data_fram2,  nvmax=19 ,nbest=1,method="exhaustive") 
regsum = summary(regfit.best)
regsum$which

print(regsum$which[1:9,])

if(dpl) pdf(file = 'UsedCars_rsq_alldat.pdf',height=10,width=12)
plot(regsum$rsq,xlab='num var (k)',ylab='R-squared',cex.lab=1.5,type='b',col='blue')
if(dpl) dev.off()

```

As expected, r-sqaured error is monotonically increasing as the number of variables increase. 

The following function gives the coefficients for any choice of variables. For instance, if we happen to choose the best subset with 5 varaibles, we use

```{r}
print(regsum$which[1:5,])
print(coef(regfit.best,3))
```
## function to do rmse for $k$ in $1:p$

```{r}
dovalbest = function(object,newdata,ynm)
  #object: regsubsets on train
  #newdata: test data
  #ynm: name of y in data frame.
{
  form = as.formula(object$call[[2]])
  p=ncol(newdata)-1
  rmsev = rep(0,p)
  test.mat = model.matrix(form,newdata)
  for(k in 1:p) {
    coefk = coef(object,id=k)
    xvars = names(coefk)
    pred = test.mat[,xvars] %*% coefk
    rmsev[k] = sqrt(mean((newdata[[ynm]]-pred)^2))
  }
  return(rmsev)
}

```

## Do validation approach several times

```{r}

ntry=100
p=ncol(data_fram2)-1
resmat = matrix(0,p,ntry) 
set.seed(18)
for(i in 1:ntry) {
  train = sample(1:nrow(data_fram2),floor(nrow(data_fram2)/2))
  regfit.best=regsubsets(price~.,data=data_fram2[train,],nvmax=19,nbest=1,method="exhaustive")
  resmat[,i]=dovalbest(regfit.best,data_fram2[-train,],'price')
}

```
## Plot results of repeated train/val

```{r}
mresmat = apply(resmat,1,mean) #average across columns
if(dpl) pdf(file='usedCars_train-val.pdf',height=10,width=12)
plot(mresmat,xlab='num vars',ylab='rmse',type='b',col='blue',pch=19,cex.lab=1.5)
if(dpl) dev.off()
```
## Fit using number of vars chosen by train/validation and all the data.

```{r}
kopt = 3 #optimal k=number of vars: chosen by eye-balling plot
regfit.best=regsubsets(price~.,data=data_fram2,nvmax=kopt,nbest=1,method="exhaustive")
xmat = model.matrix(data_fram2$price~.,data_fram2)
ddf = data.frame(xmat[,-1],price=data_fram2$price) #don't use intercept (-1), and in y=price
nms = c(names(coef(regfit.best,kopt))[-1],"price")
ddfsub = ddf[,nms] #drop all vars except those names by the coef at kopt
thereg = lm(price~.,ddfsub)
print(summary(thereg))
```

## BIC

```{r}
regfit.best=regsubsets(price~.,data=data_fram2,nvmax=9,nbest=1,method="exhaustive")
if(dpl) pdf(file="bic-best-on-train_usedCars.pdf",height=8,width=7)
plot(regfit.best) 
#will plot models ordered by BIC (the default)

for(i in 1:ncol(data_fram2)) abline(v=i,col="red",lty=2) 
#so you can see which var it is.
if(dpl) dev.off()

sumreg = summary(regfit.best)
if(dpl) pdf(file="usedCars-BIC.pdf",height=10,width=12)
plot(sumreg$bic,xlab="k",ylab="BIC",type='b',col="blue",lwd=2,cex.lab=1.5)
if(dpl) dev.off()
```
## Ploting the model with the best coice of number of variables.

### Using BIC and Cross validation.

By eyeballing we see that the best choice of $p$ for both is 3.
```{r}
print(coef(regfit.best,3)) #coefficients for best model with 2 x variables
```
Thus the best polynomila would be

$$
Y_i=\beta_0+\beta_1 x_i+\beta_2x^2_i+\beta_4x^4_i
$$
Where 
$$
\beta_0 = 7.176771 \times 10^{04} \\
\beta_1 = -8.152826\times 10^{-01}\\
\beta_2 = 2.771420\times 10^{-06} \\
\beta_4 = -8.457424\times 10^{-18}
$$

We plot this polynomial with data
```{r}
fittedmodel = 7.176771e+04 + (-8.152826e-01)*data_fram2$mileage+(2.771420e-06)*data_fram2$mileage2 
+ (-8.457424e-18)*data_fram2$mileage4
plot(data_fram2$mileage,data_fram2$price)
par(new=TRUE)
plot(data_fram2$mileage,fittedmodel,col ="red")
```
This looks good except for the mileage from 150000 to 20000 is a bit off. \

### Using a "way to big" $p$. 
Since in our data frame we have used the maximum degree 9, we will fit a polynomial of degree 9. 

```{r}
model_with_big_P =  6.265684e+04  + 1.835415e-01*(data_fram2$mileage)
-3.725474e-05*(data_fram2$mileage2)+8.178492e-10*(data_fram2$mileage3)
-9.556166e-15*(data_fram2$mileage4)+
6.534804e-20*(data_fram2$mileage5)-2.535158e-25*(data_fram2$mileage6) 
+ 4.917961e-31*(data_fram2$mileage7) -2.823111e-37*(data_fram2$mileage8)
-2.254280e-43*(data_fram2$mileage9) 
plot(data_fram2$mileage, data_fram2$price)
par(new=TRUE)
plot(data_fram2$mileage, model_with_big_P,col ="red")
```

This one is not as good as the previous and slight off for most of the data but does capture the points near the boundary vary well. 










