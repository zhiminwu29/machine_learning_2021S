---
title: '598: Machine Learning Project'
subtitle: 'Application of Classification Methods on Spooftify Data'
author: "Antonio Campbell, Sinta Sulistyo, Atta Ullah, Penny Wu"
date: "4/30/2021"
output:
  pdf_document:
    includes: 
      in_header: "packages.tex"
  pdf_notebook: default
---

```{r setup, include=FALSE}
#=============================================
## Clear .Rdata
rm(list=ls())

## Packages
pkg.list <-c("randomForest", "gbm", "rpart", 
             "rpart.plot","keras", "tensorflow", 
             "Metrics", "ggplot2", "tidyverse",
             "corrplot","rpart", "MASS", 
             "rpart.plot", "randomForest", 
             "randomForestExplainer",
             "corrr", "Hmisc", "corrplot",
             "knitr", 'nnet', 'caret',
             'reshape2', 'ROCR')

#install.packages(pkg.list)
lapply(pkg.list, require, character.only = TRUE)

knitr::opts_chunk$set(echo = FALSE, fig.align="center")
#=============================================
```

```{r, echo = FALSE}
df = read.csv("project_data.csv")
```
# 1 Introduction

In this project we have considered a dataset of 2017 songs from Spotify. This data was provided by Spotify and posted by user GeorgeMcIntire on Kaggle ("https://www.kaggle.com/geomack/spotifyclassification"). Within the data, each song has 16 features. The feature of interest for classification is called `target` which indicates whether the creator of the dataset liked or disliked a song. A song is labeled "1" if it is liked and "0" when it is disliked. The other features include  `acousticness`, `danceability`, `durationMs (duration in milliseconds)`, `energy`, `instrumentalness`, `key`, `liveness`, `loudness`, `mode`, `speechiness`, `tempo`, `time-signature`, `valence`, `songname`, `artist`.

The goal of the project is to build several classifiers for prediction that is based on the rest of the features to determine whether the individual would like a song. We begin by preparing the data to fit possible models appropriately. 

# 2 Data exploration and feature selection:
```{r}
t1 = cbind(head(df[,1:2]),...="..." ,head(df[,15:17]))
t2 = cbind(tail(df[,1:2]),...="..." ,tail(df[,15:17]))
t1$artist = as.character(t1$artist)
t2$artist = as.character(t2$artist)
t1$song_title = as.character(t1$song_title)
t2$song_title = as.character(t2$song_title)
t1$... = as.character(t1$...)
t2$... = as.character(t2$...)
```

```{r, results='asis'}
kable(rbind(t1, rep('\\vdots', 6) ,t2), 
      row.names = FALSE,  escape = FALSE, align = 'c', caption = "Data Sample")
```

#### Data Preparation 

We ensure the data is adequate for fitting before we begin modeling. In particular we are looking for features which have missing values, the songs that are duplicated in the dataset, the types of features (if a feature is numerical or categorical), and as well as the variables of importance for the fit. 

```{r}
print1 = t(data.frame(dim(df)))
rownames(print1) <- c("count")
colnames(print1) <- c("rows", "columns")
kable(print1, caption = "Data Dimensions")
```

We drop the first column, which is just for indexing and has no use to us here. The data has the remaining features

```{r}
df1 = df[,c(2:ncol(df))]
colnames(df1)
```

```{r}
#summary(df)
isNull = is.null(df1)

if(isNull == FALSE)
{
  a = 0
}
```

Next we check if there are any missing values within the data and we find that there are `r a` records with missing data in them; however, there are 5 duplicate records:

```{r, echo = FALSE}
dupeDat = df1[which(duplicated(df1)),]
kable(cbind(dupeDat[,1:2], ...="..." ,dupeDat[,15:16]))
```

We remove the 5 remaining data points and the data that remains has the following dimensions, 
```{r}
df2 = unique(df1)
print2 = t(data.frame(dim(df2)))
rownames(print2) <- c("count")
colnames(print2) <- c("rows", "columns")
kable(print2, caption = "Data Dimensions")
```

#### Correlation with Target (Correlation Matrix Heatmap): 

We include a correlation heat plot. For this analysis we have droped the last two features 'song_title' and 'artist_name', which we do not use in our final analysis as well as those previously dropped.  

```{r, message = FALSE, warning = FALSE}
df2_cor = cor(as.matrix(df2[,1:14]))
corrplot(df2_cor, method="color",type = "lower")
#df2_cor
```
The results of this plot tells us that `instrumentalness`, `danceability`, `speechiness`, and `acousticness` are among the most correlated with the the target variable. 

#### Correlation of other feautres with 'target':


# 3 Features selection based on correlations with 'target':
###### acousticness
###### danceability				
###### duration_ms				
###### instrumentalness				
###### speechiness	
###### valence


```{r}
df4 = df3[,c(14,1,2,3,5,10,13)]
head(df4)
ncol(df4)
```

#4 Models  
The potential methods to build a calcification model for this project include:

## Logistic Regression (Penny)

## Naive Bayes (Sinta)

## Decision Trees (Atta)

## Random Forests (Sinta)

## Bossting (Penny)

## Test Train split
```{r}
n = nrow(df4)
set.seed(99)
pin = .75
ii = sample(1:n,floor(pin*n))
cdtrain = df4[ii,]
cdtest = df4[-ii,]
cat("dimension of train data:",dim(cdtrain),"\n")
## dimension of train data: 750 3
cat("dimension of test data:",dim(cdtest),"\n")
## dimension of test data: 250 3

```

```{r}
fit = rpart(target~., data = cdtrain, method = 'class')
#rpart.plot(fit, extra = 106)
```

```{r}
predict_unseen = predict(fit, cdtest, type = 'class')
table_mat = table(cdtest$target, predict_unseen)
table_mat
```

```{r}
accuracy_Test = sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for test data is: ', accuracy_Test))
```
## Neural Nets
```{r}
size = seq(5, 100, by = 5)
decay = c(0.0001,0.001, 
          seq(0.0025, 0.01, by = 0.0025), 
          .025, 0.05, 0.075,0.1, 0.25, 0.5)

expGrid = expand.grid(size = seq(5, 100, by = 5), 
                   decay = c(0.0001,0.001, seq(0.0025, 0.01, by = 0.0025), 
                             0.025, 0.05, 0.075,0.1, 0.25, 0.5))
```

We begin with the simplest model we can fit based on the three main predictors that we have used for the previous models, `danceability`, `speachiness`,  `acousticness`. We will use a single-layer neural network to fit on the training data. For optimization we will begin by using a grid search accross of layer size. We also want to introduce regularization through weight decay so we will include those in our grid as well. The values traversed through are:

```{r}
kable(t(size), caption = "Size Grid")
kable(t(decay), caption = "Decay Grid")
```

```{r}
#functions to scale on 0-1
scaling01 <- function(x){
  (x-min(x))/(max(x)-min(x))
}

scaleCols <-function(x){
  a = matrix(1, nrow = nrow(x))%*%apply(x,2,min)
  b = matrix(1, nrow = nrow(x))%*%apply(x,2,max)
  return((x - a)/(b-a))
}
```

```{r}
#copies for scaling and scale them
sTrain = scaleCols(cdtrain)
sTest = scaleCols(cdtest)
sTrain$target = as.factor(sTrain$target)
sTest$target = as.factor(sTest$target)
```

```{r}
#create new train/validation split for initial search
trainValsplits <- function(i = 10, vpercent){
  set.seed(i)
  trainN = nrow(sTrain)
  verifyInd = sample(1:trainN,floor(vpercent*trainN))
  return(verifyInd)
}
```

```{r}
#function to train on training data for various fits and verify
fitModel <- function(dat, grid, verifyInd)
{
  nV= dat[verifyInd,]
  nT = dat[-verifyInd,]
   
  grid1 = cbind(grid, acc=1, sens=1, spec=1)
  for(i in 1:nrow(grid)){
    fit = nnet(target ~ danceability+speechiness+acousticness, 
               nT, size = grid[i,1], decay = grid[i,2], linout=FALSE, 
               trace = FALSE)
    outpred = data.frame(rawProbs = predict(fit, nV,c("raw")))
    outpred$class = as.factor(predict(fit, nV,c("class")))
    z = confusionMatrix(nV$target,outpred$class)
    grid1[i,4:5] = z$byClass[1:2]
    grid1[i,3] = z$overall[1]
  }
  return(grid1)
}

```

```{r}
#choose columns for simple fit
nnTrain1 = sTrain[,c("target","danceability","speechiness", "acousticness")]
nnTest1 = sTest[,c("target","danceability","speechiness", "acousticness")]
```

```{r, cache = TRUE}
#interesting seeds: 192, 1959, 

#create test and validation splits and view them at
#all combinations oof parameters
split = trainValsplits(i = 1959, vpercent = .25)
outGrid = fitModel(dat = nnTrain1, grid = expGrid, verifyInd = split)
 
#order outputs from large grid 
ord = order(outGrid$acc, decreasing = TRUE)
topFits = outGrid[ord[1],]
midFits = outGrid[c(ord[(floor(length(ord)/4))],ord[(floor(length(ord)/2))],ord[3*(floor(length(ord)/4))]),]
lastFit = outGrid[ord[length(ord)],]
fits = rbind(topFits,midFits,lastFit)
```

For an initial grid optimization based on the expansion of the grid over sizes/decay rates, we hold out 25% of the data training data for a validation set and compare the methods based on accuracy after fitting on what remains in the training set. This gives us a basis of what parameters will work well, but we will see that these can be unrelaible and very biased to the data. We sort the fits based on their accuracy, and then use the following candidates as parameters from the following fits for comparison:  the first two best fits, the 25th/50th/75th percentiles, and the worst fit. These are:

```{r}
kable(fits, caption = "Various Parameters Considered", row.names =FALSE)
```
The following box plots show how these parameters fair against eachother over various random splits of the train/validation splits based on accuracy, specificity, and sensitiviy. We iterate serveral times to obtain these distributions. 

```{r, fig.height= 8, cache = TRUE, message="FALSE", warn = "FALSE"}
runs = 10 #500 #set low to save time, change to 250-500 in last compilation, takes like 5 mins to run
keep = runs*nrow(fits)
keeps = matrix(0, nrow = keep, ncol = 4)
keeps = data.frame(keeps)
keeps[,1] = rep(paste("Size",fits$size,"Rates",fits$decay, sep = ""),runs)

for(j in 1:runs){
  split1 =  trainValsplits(j, vpercent = .25)
  outGrid = fitModel(dat = nnTrain1, grid = fits[,1:2], verifyInd = split1)
  keeps[(j*5-4):(j*5),2:4] = outGrid[,3:5]
}

keeps$X1 = as.factor(keeps$X1)
colnames(keeps)  <- c("SizeRate", "Accuracy", "Sensitivity", "Specificity")
keepsMelt = melt(keeps, 
                 id.variables = c("SizeRate"), 
                 variable.name = "Measure", 
                 value.name = "Percentage")

ggplot(keepsMelt, aes(x = SizeRate, y = Percentage)) + 
  geom_boxplot() + 
  facet_wrap( ~Measure , ncol = 1, scales = "free_y")
```

Over `r runs` interations we have a comparison of these methods, and we see that what we though would provide parameters for the best fit do not always appear to fit just as well. While our prediction problem is not a matter of life or death, we conside measures like specificity/sensitivity can be used to choose the parameters. Intuitively, It may be the case that we can identify who likes quite a bit of music in general, and we are more concerned with classifying the songs that they wouldn't like, rather than they would. Then it would be logical to include specificity in our search for good parameters. The oppositive can be said about people who are very picky about music. We include an ROC plot of the fit models based on their fits. 

```{r}
#ROC Plot Code
plotROC <- function(pihat, ytrue, add = TRUE, col = '#791b19') {
  thresh <- sort(pihat)
  N <- length(pihat)
  yhat <- sapply(1:N, function(a) as.double(pihat >= thresh[a]))
  tpr <- sapply(1:N, function(a) length(which(ytrue == 1 & yhat[,a ] == 1)) / sum(ytrue == 1)) 
  fpr <- sapply(1:N, function(a) length(which(ytrue == 0 & yhat[,a ] == 1)) / sum(ytrue == 0)) 
  if (add == FALSE) {
    plot(fpr,tpr, pch = 20, cex = 0.2,alpha= 0.3, col = col, bty = 'n', type = 'b', xlim = c(0,1), ylim = c(0,1), 
         main = "ROC Plot", ylab = "True Positive Rate", xlab = "False Posive")
  abline(a = 0, b = 1, lty = 2, col = col, bty = 'n') }
  else {
  points(fpr, tpr, pch = 20, cex = 0.1,  col = c(col,alpha = 0.4), bty = 'n', type = 'l')
  points(fpr, tpr, pch = 20, cex = 0.3, col = col, bty = 'n') }
}
```

```{r}
fit1 = nnet(target ~ danceability+speechiness+acousticness, 
               nnTrain1, size = fits[1,1], decay = fits[1,2], linout=FALSE, 
               trace = FALSE)
fit2 = nnet(target ~ danceability+speechiness+acousticness, 
               nnTrain1, size = fits[2,1], decay = fits[2,2], linout=FALSE, 
               trace = FALSE)
fit3 = nnet(target ~ danceability+speechiness+acousticness, 
               nnTrain1, size = fits[3,1], decay = fits[3,2], linout=FALSE, 
               trace = FALSE)
fit4 = nnet(target ~ danceability+speechiness+acousticness, 
               nnTrain1, size = fits[4,1], decay = fits[4,2], linout=FALSE, 
               trace = FALSE)
fit5 = nnet(target ~ danceability+speechiness+acousticness, 
               nnTrain1, size = fits[5,1], decay = fits[5,2], linout=FALSE, 
               trace = FALSE)

outpred1 = data.frame(rawProbs = predict(fit1, nnTest1,c("raw")))
outpred2 = data.frame(rawProbs = predict(fit2, nnTest1,c("raw")))
outpred3 = data.frame(rawProbs = predict(fit3, nnTest1,c("raw")))
outpred4 = data.frame(rawProbs = predict(fit4, nnTest1,c("raw")))
outpred5 = data.frame(rawProbs = predict(fit5, nnTest1,c("raw")))
preds = cbind(outpred1,outpred2,outpred3,outpred4,outpred5)


#colors and plot
cO = c("#000080","#791b19", "#DAA520", "#228B22", "#A9A9A9")
{
  plot(0, col = "white",xlim = c(0,1), ylim = c(0,1),
  main = "ROC Plot", ylab = "True Positive Rate", xlab = "False Positive") 
  abline(a = 0, b = 1, lty = 2, col = "#000000", bty = 'n')
  
  for (j in 1:length(fits$size)){
    plotROC(preds[,j], nnTest1 , add = TRUE, col = cO[j])
  } 
  legend("bottomright", 
  legend = c("Fit1", "Fit2", "Fit3", "Fit4", "Fit5"), 
  col = cO,
  pch = c(17,17,17,17,17), 
  bty = "n", 
  pt.cex = 1, 
  cex = 1, 
  text.col = "black", 
  horiz = F , 
  inset = c(0.1, 0.1))
}
```

It appears that predictions for most methods look apporximately the same. The only method that falls completely off from the mark is the very last one. We do see regions where our supposed 'best fit' from our initial serach would not have the best accuracy, but the difference isn't extreme. 

```{r, results = 'asis'}
#Test prediction accuracy
z1 = confusionMatrix(as.factor(ifelse((preds[,1])>0.5,"1","0")),
                     nnTest1$target)$overall[1]
z2 = confusionMatrix(as.factor(ifelse((preds[,2])>0.5,"1","0")),
                     nnTest1$target)$overall[1]
z3 = confusionMatrix(as.factor(ifelse((preds[,3])>0.5,"1","0")),
                     nnTest1$target)$overall[1]
z4 = confusionMatrix(as.factor(ifelse((preds[,4])>0.5,"1","0")),
                     nnTest1$target)$overall[1]
z5 = confusionMatrix(as.factor(ifelse((preds[,5])>0.5,"1","0")),
                     nnTest1$target)$overall[1]
testFits = cbind(fits[,1:2], accuracy = c(z1,z2,z3,z4,z5)) 
orderFits = order(testFits$accuracy, decreasing = TRUE)
kable(testFits[orderFits,], caption = "Test Accuracy")
```

```{r}
# Performance
# perf1= performance(prediction(
#   predict(fit1, nnTest1,c("raw")), nnTest1$target), "tpr","rpp")
# p3erf2= performance(prediction(
#   predict(fit2, nnTest1,c("raw")), nnTest1$target), "tpr","rpp")
# perf3= performance(prediction(
#   predict(fit3, nnTest1,c("raw")), nnTest1$target), "tpr","rpp")
# perf4= performance(prediction(
#   predict(fit4, nnTest1,c("raw")), nnTest1$target), "tpr","rpp")
# perf5= performance(prediction(
#   predict(fit5, nnTest1,c("raw")), nnTest1$target), "tpr","rpp")
# lperfs = list(perf1,perf2,perf3,perf4,perf5)
# 
# {
#   plot(0, col = "white",xlim = c(0,1), ylim = c(0,1),
#   main = "Cummulative Gains", ylab = "Treu Positive Rate", xlab = "Rate of Positive Predictions") 
#   abline(a = 0, b = 1, lty = 2, col = "#000000", bty = 'n')
#   for(i in 1:length(lperfs)){
#     plot(lperfs[[i]], add = TRUE, colorize = FALSE, col = cO[i])
#   }
#   legend("bottomright", 
#   legend = c("Fit1", "Fit2", "Fit3", "Fit4", "Fit5"), 
#   col = cO,
#   pch = c(17,17,17,17,17), 
#   bty = "n", 
#   pt.cex = 1, 
#   cex = 1, 
#   text.col = "black", 
#   horiz = F , 
#   inset = c(0.1, 0.1))
# } 
```

Now we consider how neural networks compare when we use all of the predictors rather than just the three we originally looked at. We fit on various parameters again and compare different candidates based on their accuracy between random train/validation splits witihin the allocated training data. We first noticed that the fits improved immediately by including other predictors, but the same problem arose where we overfit to the data and even though our validation set pointed us in the direction of the top parameters, they weren't great when testing. Just choosing the worst, best, and candidates in between so we tried something else. 

Instead of just picking candidates only from an initial train/validation split, we start with our initial fits remove a fourth of the candidates iteratively by sorting the carndidates on accuracy on the current validation set, we split the training data once again and add the new accuracy too the current after refitting on this split. We repeat the proceedure until only 5 parameter combinations are left and then we average over the values of accuracy and sort once again for our choice of the top 5 models. The following are the results we ended up with for candidates and their ROC curves. We also include parameters chosen at the percentiles of accuracy again from the first train/validatioon split. 

```{r}
#function to train on training data for various fits and verify
fitModel <- function(dat, grid, verifyInd)
{
  nV= dat[verifyInd,]
  nT = dat[-verifyInd,]
   
  grid1 = cbind(grid, acc=1, sens=1, spec=1)
  for(i in 1:nrow(grid)){
    fit = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence, 
               nT, size = grid[i,1], decay = grid[i,2], linout=FALSE, 
               trace = FALSE)
    outpred = data.frame(rawProbs = predict(fit, nV,c("raw")))
    outpred$class = as.factor(predict(fit, nV,c("class")))
    z = confusionMatrix(nV$target,outpred$class)
    grid1[i,4:5] = z$byClass[1:2]
    grid1[i,3] = z$overall[1]
  }
  return(grid1)
}
```

```{r, cahce = TRUE}
# nnTrain2 = sTrain
# nnTest2 = sTest
# 
# #create test and validation splits and view them at
# #all combinations oof parameters
# outGrid = fitModel(dat = nnTrain2, grid = expGrid, verifyInd = split)
# outGrid1 = outGrid 
# count = 1
# for (i in 1:20){
#   if(nrow(outGrid) > 6){
#     nleave = floor(nrow(outGrid)/4)
#     ord = order(outGrid$acc, decreasing = TRUE)
#     left = outGrid[ord,] 
#     left1 = left[(1:(nrow(left)-nleave)),]
#     outGrid = left1 
#     outGrid[,3:5] = left1[,3:5] + fitModel(dat = nnTrain2, grid = left1[,1:2], verifyInd = trainValsplits(count, vpercent = .25))[,3:5]
#     count = count + 1
#   }
# }
# 
# fits = outGrid
# fits[,3:5] = fits[,3:5]/count
# fits = fits[1:5,]
# 
# kable(fits)
```

Using candidates at the same positions before

```{r}
# fit1 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence,
#                nnTrain2, size = fits[1,1], decay = fits[1,2], linout=FALSE, 
#                trace = FALSE)
# fit2 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence,
#                nnTrain2, size = fits[2,1], decay = fits[2,2], linout=FALSE, 
#                trace = FALSE)
# fit3 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence, 
#                nnTrain2, size = fits[3,1], decay = fits[3,2], linout=FALSE, 
#                trace = FALSE)
# fit4 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence, 
#                nnTrain2, size = fits[4,1], decay = fits[4,2], linout=FALSE, 
#                trace = FALSE)
# fit5 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence, 
#                nnTrain2, size = fits[5,1], decay = fits[5,2], linout=FALSE, 
#                trace = FALSE)
# 
# outpred1 = data.frame(rawProbs = predict(fit1, nnTest2,c("raw")))
# outpred2 = data.frame(rawProbs = predict(fit2, nnTest2,c("raw")))
# outpred3 = data.frame(rawProbs = predict(fit3, nnTest2,c("raw")))
# outpred4 = data.frame(rawProbs = predict(fit4, nnTest2,c("raw")))
# outpred5 = data.frame(rawProbs = predict(fit5, nnTest2,c("raw")))
# preds = cbind(outpred1,outpred2,outpred3,outpred4,outpred5)
# 
# 
# #colors and plot
# cO = c("#000080","#791b19", "#DAA520", "#228B22", "#A9A9A9")
# {
#   plot(0, col = "white",xlim = c(0,1), ylim = c(0,1),
#   main = "ROC Plot", ylab = "True Positive Rate", xlab = "False Positive") 
#   abline(a = 0, b = 1, lty = 2, col = "#000000", bty = 'n')
#   
#   for (j in 1:length(fits$size)){
#     plotROC(preds[,j], nnTest1 , add = TRUE, col = cO[j])
#   } 
#   legend("bottomright", 
#   legend = c("Fit1", "Fit2", "Fit3", "Fit4", "Fit5"), 
#   col = cO,
#   pch = c(17,17,17,17,17), 
#   bty = "n", 
#   pt.cex = 1, 
#   cex = 1, 
#   text.col = "black", 
#   horiz = F , 
#   inset = c(0.1, 0.1))
# }
```

```{r}
#Test prediction accuracy
# z1 = confusionMatrix(as.factor(ifelse((preds[,1])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z2 = confusionMatrix(as.factor(ifelse((preds[,2])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z3 = confusionMatrix(as.factor(ifelse((preds[,3])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z4 = confusionMatrix(as.factor(ifelse((preds[,4])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z5 = confusionMatrix(as.factor(ifelse((preds[,5])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# testFits = cbind(fits[,1:2], accuracy = c(z1,z2,z3,z4,z5)) 
# orderFits = order(testFits$accuracy, decreasing = TRUE)
# kable(testFits[orderFits,], caption = "Test Accuracies")
```


```{r}
# ord = order(outGrid1$acc, decreasing = TRUE)
# topFits = outGrid1[ord[1],]
# midFits = outGrid1[c(ord[(floor(length(ord)/4))],ord[(floor(length(ord)/2))],ord[3*(floor(length(ord)/4))]),]
# lastFit = outGrid1[ord[length(ord)],]
# fits = rbind(topFits,midFits,lastFit)
```

```{r}
# fit1 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence,
#                nnTrain2, size = fits[1,1], decay = fits[1,2], linout=FALSE, 
#                trace = FALSE)
# fit2 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence,
#                nnTrain2, size = fits[2,1], decay = fits[2,2], linout=FALSE, 
#                trace = FALSE)
# fit3 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence, 
#                nnTrain2, size = fits[3,1], decay = fits[3,2], linout=FALSE, 
#                trace = FALSE)
# fit4 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence, 
#                nnTrain2, size = fits[4,1], decay = fits[4,2], linout=FALSE, 
#                trace = FALSE)
# fit5 = nnet(target ~ danceability+speechiness+acousticness+duration_ms+instrumentalness+valence, 
#                nnTrain2, size = fits[5,1], decay = fits[5,2], linout=FALSE, 
#                trace = FALSE)
# 
# outpred1 = data.frame(rawProbs = predict(fit1, nnTest2,c("raw")))
# outpred2 = data.frame(rawProbs = predict(fit2, nnTest2,c("raw")))
# outpred3 = data.frame(rawProbs = predict(fit3, nnTest2,c("raw")))
# outpred4 = data.frame(rawProbs = predict(fit4, nnTest2,c("raw")))
# outpred5 = data.frame(rawProbs = predict(fit5, nnTest2,c("raw")))
# preds = cbind(outpred1,outpred2,outpred3,outpred4,outpred5)
# 
# 
# #colors and plot
# cO = c("#000080","#791b19", "#DAA520", "#228B22", "#A9A9A9")
# {
#   plot(0, col = "white",xlim = c(0,1), ylim = c(0,1),
#   main = "ROC Plot", ylab = "True Positive Rate", xlab = "False Positive") 
#   abline(a = 0, b = 1, lty = 2, col = "#000000", bty = 'n')
#   
#   for (j in 1:length(fits$size)){
#     plotROC(preds[,j], nnTest1 , add = TRUE, col = cO[j])
#   } 
#   legend("bottomright", 
#   legend = c("Fit1", "Fit2", "Fit3", "Fit4", "Fit5"), 
#   col = cO,
#   pch = c(17,17,17,17,17), 
#   bty = "n", 
#   pt.cex = 1, 
#   cex = 1, 
#   text.col = "black", 
#   horiz = F , 
#   inset = c(0.1, 0.1))
#}
```

```{r}
#Test prediction accuracy
# z1 = confusionMatrix(as.factor(ifelse((preds[,1])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z2 = confusionMatrix(as.factor(ifelse((preds[,2])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z3 = confusionMatrix(as.factor(ifelse((preds[,3])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z4 = confusionMatrix(as.factor(ifelse((preds[,4])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# z5 = confusionMatrix(as.factor(ifelse((preds[,5])>0.5,"1","0")),
#                      nnTest1$target)$overall[1]
# testFits = cbind(fits[,1:2], accuracy = c(z1,z2,z3,z4,z5)) 
# orderFits = order(testFits$accuracy, decreasing = TRUE)
# kable(testFits[orderFits,], caption = "Test Accuracy")
```

####Summary oof metrics of thre 

# 5 Conclusions


