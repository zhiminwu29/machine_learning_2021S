---
title: "Project"
output:
  pdf_document: default
  pdf_notebook: default

---



```{r}
library(tidyverse,corrplot)
library(rpart)
library(MASS)
library(rpart.plot)
library(randomForest)
library(randomForestExplainer)
library(corrr)
library(ggplot2)
#library(Hmisc)
library(corrplot)
library(pROC)
library(gbm)
library(caret)

df = read.csv("project_data.csv")
```
# 1 Introduction


In this project we have considered a dataset of 2017 songs from Spotify and each song has 16 features. One of the features is called target which is a categorical variable and tells us whether one particular individual liked or disliked a song. A song is labeled "1" if it is liked and "0" when it is disliked. The other features include  acousticness, danceability, durationms (duration in milliseconds), energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, timesignature, valence, songname, artist.

The goal of the project is to build several classifier to predict that based on the rest of the features, whether or not that individual would like a song. 

But first, we will prepare the data to fit some of these models.

# 2 Data exploration and feature selection:

Before we try any model, we want to make sure the data is ready for a fit. In particular we are looking for features that have missing values, songs that are duplicated in the dataset, the types of features (if a feature is numerical or categorical),  and as well as the variables of importance for the fit. 

```{r}

ncol(df)
nrow(df)
colnames(df)
```
We drop the first column, which is just an indexing column that keeps track of the number of observations and has nothing to do with the data. 

```{r}
df1 = df[,c(2:ncol(df))]
colnames(df1)


```

Looking for missing values
```{r}
#summary(df)
is.null(df1)
```

Looking for duplicated observations in the dataset. 


```{r}
mean(duplicated(df1))
```
Non-zero mean suggest that there are some duplicated observations. We now delete those.
```{r}
df2 = unique(df1)
mean(duplicated(df2))
```
Number of observations after the removal of duplicated data points. 

```{r}
nrow(df2)

```
Originally we had 2017 data points, so this means five of the data points were duplicated. 

#### Looking for correllations (Heatmap of Correlation Matrix): 

We have drooped the last two features 'song_title' and 'artist_name' as well.

```{r}
df2_cor = cor(as.matrix(df2[,1:14]))
corrplot(df2_cor, method="color",type = "lower")
#df2_cor
```


#### Looking for correlation of other feautre with 'target':
```{r}

df3 = df2[,1:14]

df3_cor <- df3 %>% 
  correlate() %>% 
  focus(target)
df3_cor
```

#### Variation in the data
\
Let's take a look at our response variable `target`. The proportion of 1s and 0s are almost half and half so we do not need to worry about balance the data for fitting.


```{r collapse=TRUE}
      table(df3$target)
      
```
```{r collapse = TRUE}

df3$target <- as.character(df3$target)
summary(df3)

```
**Covariation between variables**
We also want to have a look at how other variables may vary with `target`:\
There is some interesting covariation between some features and target, such as acounstincness, duration time, where the mode of density plots show some correlations with `target`.

```{r echo=FALSE, cache=TRUE,message=FALSE,warning=FALSE,fig.width=7, fig.height=4, fig.align="center"}
    #filter out the NAs in grade
        ggplot(df3,aes(x=target,y = acousticness)) + 
         geom_boxplot()
      

      ggplot(df3,aes(x=target,y = danceability)) + 
         geom_boxplot()
      
     ggplot(df3,aes(fill = target,x=acousticness)) + 
         geom_density(alpha=0.3)
     
    
     
      ggplot(df3,aes(fill = target,x=duration_ms)) + 
         geom_density(alpha=0.3)
      
      
      
      ggplot(df3,aes(fill = target,x=speechiness)) + 
         geom_density(alpha=0.3)
       
          
      
          

```
**variables that don't show much interesting variation against target**
```{r echo=FALSE, cache=TRUE,message=FALSE,warning=FALSE,fig.width=7, fig.height=4, fig.align="center"}
     
    ggplot(df3,aes(fill = target,x=danceability)) + 
        geom_density(alpha=0.3)
     
      ggplot(df3,aes(fill = target,x=instrumentalness)) + 
         geom_density(alpha=0.3)
      #summary(df3$instrumentalness)
      
      ggplot(df3,aes(fill = target,x=valence)) + 
         geom_density(alpha=0.3)
       

```

# 3 Features selection based on correlations with 'target':
###### acousticness
###### danceability				
###### duration_ms				
###### instrumentalness				
###### speechiness	
###### valence

We also rescale the varibale `duration_ms` so that it's on the same scale as other variables.

```{r}
df4 = df3[,c(14,1,2,3,5,10,13)]
df4$duration_ms <- (df4$duration_ms-min(df4$duration_ms))/(max(df4$duration_ms-min(df4$duration_ms)))
head(df4)
#ncol(df4)

```

#4 Models  
The potential methods to build a calcification model for this project include:

## Logistic Regression (Penny)
## Naive Bayes (Sinta)
## Decision Trees (Atta)
## Random Forests (Sinta)
## Boosting (Penny)
## Neural Nets (Antonio)
## Deep Neural Nets (Antonio)

## Test Train split
```{r}
n = nrow(df4)
set.seed(199)
pin = .65
ii = sample(1:n,floor(pin*n))
cdtrain = df4[ii,]
cdtest = df4[-ii,]
cat("dimension of train data:",dim(cdtrain),"\n")
## dimension of train data: 750 3
cat("dimension of test data:",dim(cdtest),"\n")
## dimension of test data: 250 3

```
```{r}
fit = rpart(target~., data = cdtrain, method = 'class')
#rpart.plot(fit, extra = 106)

```

```{r}
predict_unseen = predict(fit, cdtest, type = 'class')
table_mat = table(cdtest$target, predict_unseen)
table_mat
```

```{r}
accuracy_Test = sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for test data is: ', accuracy_Test))
```



## Logistic regression
\
We want to start out simple, so let's use only one predictor to fit the training set.
```{r collapse=TRUE}
      glm.fits=glm(as.factor(target)~speechiness,
                   data=cdtrain,family=binomial)
      contrasts(as.factor(cdtrain$target))
      summary(glm.fits)
      
```
**Evaluation of the model**
\
We first plot the ROC curve on the train set and the AUC is about 60%, but the AUC of the test set decreased a little, which is 57.5%. 
```{r echo=FALSE,include=TRUE,message=FALSE,fig.width=7,fig.height=4,warning=FALSE,fig.align="center"}

    cdtrain%>%
      mutate(prob=glm.fits$fitted.values)%>%
      ggplot(aes(x=acousticness,y=prob))+
      geom_smooth(se=FALSE)+
      labs(y='Probability(1)')
    
    #ROC curve
    par(pty='s')
    
    roc(cdtrain$target, glm.fits$fitted.values,
            plot=TRUE,legacy.axes=TRUE, percent=TRUE, 
            xlab="False Positive Percentage", ylab="True Postive Percentage",print.auc=TRUE)

```


\
Confusion matrix and performance statistics on the test set:
```{r }
    
    glm.probs=predict(glm.fits,newdata=cdtest,type="response")
    str(glm.probs)
  
    glm.pred=rep('0',length(glm.probs))
    glm.pred[glm.probs >.5]='1'
    
    #confusion matrix
    #table(glm.pred,cdtest$target)
    # accuracy
    #(176+83)/length(glm.probs) 
   
    confusionMatrix(as.factor(glm.pred),as.factor(cdtest$target))
```

Here is the ROC for the test results:
```{r echo=FALSE, cache=TRUE,fig.width=7,fig.height=4}
  #ROC curve
    par(pty='s')
    
    roc(cdtest$target, glm.probs,
            plot=TRUE,legacy.axes=TRUE, percent=TRUE, 
            xlab="False Positive Percentage", ylab="True Postive Percentage",print.auc=TRUE)
```

The performance of one-predictor logistic regression is not very ideal, with 54% accuracy on the test set. The AUC is also fairly low, 57.5%.
\

**Logistic regression with multiple predictors**\
Next we will try to use more than one predictors to fit logistic regression.
```{r collapse=TRUE,cache=TRUE}
      glm.fits2=glm(as.factor(target)~speechiness+danceability+duration_ms+
                      instrumentalness+acousticness,data=cdtrain,family=binomial)

      summary(glm.fits2)
```



**Predict on the test set and evaluate the performance.**\
We see that it does do a better job than the one-predictor model above, both accuracy and AUC are increased by significant amount but it is still not what we desire as performance. 
```{r cache=TRUE,fig.width=7,fig.height=4}
    
    glm.probs2=predict(glm.fits2,newdata=cdtest,type="response")
  
    glm.pred2=rep('0',length(glm.probs2))
    glm.pred2[glm.probs2 >.5]='1'
    
    #confusion matrix
    #table(glm.pred2,cdtest$target)
    # accuracy
    #(176+83)/length(glm.probs2) 
    confusionMatrix(as.factor(glm.pred2),as.factor(cdtest$target))
    
    #ROC CURVE
    par(pty='s')
    roc(cdtest$target, glm.probs2,
            plot=TRUE,legacy.axes=TRUE, percent=TRUE, 
            xlab="False Positive Percentage", ylab="True Postive Percentage",print.auc=TRUE)

```

## Boosting
\
Since there are three tuning parameters of the boosting model, we will use 8 different combinations of those parameters to fit the models on the train and chose the one that gives best performance on the validation data. Here are the choices for the three hyper-parameters:

* maximum depth = 4 or 10
* number of trees = 1000 or 5000
* shrinkage = 0.001 or 0.2

**Train, Validation and Test Split**
In order to use the three set approach, we divide the data into three sets, the train set, the validation set and the test set.
```{r, echo = TRUE, cache = TRUE}
set.seed(1722)
n=nrow(df4)
n1=floor(n/2)
n2=floor(n/4)
n3=n-n1-n2
ii = sample(1:n,n)
train = df4[ii[1:n1],]
val = df4[ii[n1+1:n2],]
trainval = rbind(train,val)
test = df4[ii[n1+n2+1:n3],]
```


```{r, echo = TRUE, cache = TRUE,}
set.seed(122)
boostfit1 = gbm(target~.,data = train,distribution="bernoulli",
interaction.depth = 4, n.trees = 1000,shrinkage = 0.2)
summary(boostfit1)
```

```{r, echo=TRUE, cache=TRUE}
boostfit2 = gbm(target~.,data=train,distribution = "bernoulli",
interaction.depth = 4, n.trees = 1000,shrinkage = 0.001)
boostfit3 = gbm(target~.,data=train,distribution = "bernoulli",
interaction.depth =4, n.trees = 5000,shrinkage = 0.2)
boostfit4 = gbm(target~.,data=train,distribution = "bernoulli",
interaction.depth = 4, n.trees = 5000, shrinkage = 0.001)
boostfit5 = gbm(target~.,data=train,distribution="bernoulli",
interaction.depth = 10, n.trees = 1000,shrinkage = 0.2)
boostfit6 = gbm(target~.,data=train,distribution = "bernoulli",
interaction.depth = 10, n.trees = 1000,shrinkage = 0.001)
boostfit7 = gbm(target~.,data=train,distribution = "bernoulli",
interaction.depth = 10, n.trees = 5000,shrinkage = 0.2)
boostfit8 = gbm(target~.,data=train,distribution = "bernoulli",
interaction.depth = 10, n.trees = 5000,shrinkage = 0.001)
```


Predict on the test. We choose the probability threshold to be 0.5 to classify probability greater than 0.5 as the class '1', '0' otherwise.
```{r, echo = FALSE, cache = TRUE}
bpred1 = predict(boostfit1, newdata = val, n.trees = 1000,type='response')
bpred2 = predict(boostfit2, newdata = val, n.trees = 1000,type='response')
bpred3 = predict(boostfit3, newdata = val, n.trees = 5000,type='response')
bpred4 = predict(boostfit4, newdata = val, n.trees = 5000,type='response')
bpred5 = predict(boostfit5, newdata = val, n.trees = 1000,type='response')
bpred6 = predict(boostfit6, newdata = val, n.trees = 1000,type='response')
bpred7 = predict(boostfit7, newdata = val, n.trees = 5000,type='response')
bpred8 = predict(boostfit8, newdata = val, n.trees = 5000,type='response')
```
\

We then calculate the accuracy of all models' prediction on the validation set. According to the accuracy plot, the best result belongs to the fourth model with maximum depth of 4, 5000 trees, and shrinkage of 0.001.
```{r, echo=FALSE, cache=TRUE}
preds_prob = cbind(bpred1,bpred2,bpred3,bpred4,bpred5,bpred6,bpred7,bpred8)
preds = ifelse(preds_prob>0.5,1,0)
ybin = as.factor(val$target)

ac1<-confusionMatrix(as.factor(preds[,1]),ybin)$overall['Accuracy']
ac2<-confusionMatrix(as.factor(preds[,2]),ybin)$overall['Accuracy']
ac3<-confusionMatrix(as.factor(preds[,3]),ybin)$overall['Accuracy']
ac4<-confusionMatrix(as.factor(preds[,4]),ybin)$overall['Accuracy']
ac5<-confusionMatrix(as.factor(preds[,5]),ybin)$overall['Accuracy']
ac6<-confusionMatrix(as.factor(preds[,6]),ybin)$overall['Accuracy']
ac7<-confusionMatrix(as.factor(preds[,7]),ybin)$overall['Accuracy']
ac8<-confusionMatrix(as.factor(preds[,8]),ybin)$overall['Accuracy']

p = c(1:8)
accuracy = c(ac1,ac2,ac3,ac4,ac5,ac6,ac7,ac8)
plot(p,accuracy)
```

We also calculated the AUC for all 8 models and plotted the AUCs. Agian, we confirm that the fourth model is the best. So we will use the fourth one to predict on the test set.
```{r, warnining=FALSE,cache=TRUE,echo=FALSE}


cal_auc <- function(x) {
  auc(roc(ybin,x))
  }
cal_auc(bpred1)

bpreds = cbind(bpred1,bpred2,bpred3,bpred4,bpred5,bpred6,bpred7,bpred8)

AUC<-apply(bpreds,2,cal_auc)
plot(p,AUC)

```

Predicting on the test set, we get accuracy of 0.72.
```{r, echo = TRUE, cache = TRUE}
boostfit = gbm(target~.,data = trainval ,distribution="bernoulli",
interaction.depth = 4, n.trees = 5000,shrinkage = 0.001)
boostvalpred = predict(boostfit, newdata = test, n.trees = 5000)
pred_test = ifelse(boostvalpred>0.5,1,0)
confusionMatrix(as.factor(pred_test),as.factor(test$target))$overall['Accuracy']

```
# 5 Conclusions


